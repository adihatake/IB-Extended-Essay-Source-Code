{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1yiiGSh3auTicW2AM_WSp3bSr-ATh_2e6","authorship_tag":"ABX9TyNEW6m0bejupIj/GbUPcp12"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install Dependencies"],"metadata":{"id":"x02374AlOZ_2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvVJCfDEA4ps","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1707118992356,"user_tz":420,"elapsed":8226,"user":{"displayName":"Adrian Ba","userId":"05340278167687070637"}},"outputId":"666681c9-5c01-4831-ac95-04cea3b1c1f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n"]},{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","import datetime\n","import seaborn as sn\n","import pandas as pd\n","import sklearn\n","import itertools\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from PIL import Image, ImageChops\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import confusion_matrix\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Rescaling, GlobalAveragePooling2D\n","from keras.utils import to_categorical\n","from keras import backend as K\n","from keras.callbacks import Callback\n","from keras.applications.vgg16 import VGG16\n","from IPython.display import Audio, display\n","\n","\n","dataset_path = \"/content/drive/MyDrive/Extended_Essay/Data_GZAN/images_original\"\n","\n","music_genres_list = ['blues', 'classical', 'country', 'disco',\n","                     'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n","\n","%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","tf.test.gpu_device_name()"]},{"cell_type":"markdown","source":["# Prepare Image Data"],"metadata":{"id":"NWYrYGBJOehv"}},{"cell_type":"code","source":["\n","\n","# a manual way to load image data. we will convert each image into a numpy array\n","# and load it directly to memory\n","\n","\n","def trim_white_space(im):\n","    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n","    diff = ImageChops.difference(im, bg)\n","    diff = ImageChops.add(diff, diff, 2.0, -100)\n","    bbox = diff.getbbox()\n","    if bbox:\n","        return im.crop(bbox)\n","    else:\n","        # Failed to find the borders, convert to \"RGB\"\n","        return trim_white_space(im.convert('RGB'))\n","\n","\n","def get_images(input_images_dir):\n","  num_images = len(os.listdir(input_images_dir))  # find number of images in dataset\n","  data_loader_size = (288, 432)   #specify input image size for keras data loader\n","  final_image_size = (218, 336)  # specify final image size for the dataset\n","  dataset = np.zeros(shape=(num_images, final_image_size[0], final_image_size[1], 3))  # creates empty array of batches size (1000, 218, 336, 3)\n","                                                                           # indicates (num_imgs, image_dim1, image_dim2, #of channels like rgb)\n","\n","  for image_index, file_name in enumerate(os.listdir(input_images_dir)):  # enumerate listing of image_dir\n","    image_path = f\"{input_images_dir}/{file_name}\"\n","    print(image_path)\n","\n","    # load img from directory as a PIL instance\n","    img = keras.utils.load_img(\n","        path=image_path,\n","        color_mode='rgb',\n","        target_size=data_loader_size,\n","        interpolation='bilinear') # what is interpolation? --> helps rescale image if it doesn't fit with the target size\n","\n","\n","    img = trim_white_space(img) # trim surrounding white space in the image\n","\n","    img = keras.utils.img_to_array(img) # converts PIL instance from utils.load_img() to numpy array\n","    img = img.astype(\"float32\") / 255 # scale pixel values to range of 0-1 to make model training easier\n","    dataset[image_index] = img # replace specific index of dataset to numpy image array\n","\n","  return np.array(dataset) # return array of the dataset\n","\n","\n","\n","def get_dataset(dataset_dir):\n","  input_data = [] # create some empty variables\n","  target_labels = []\n","  genre_index = 0\n","\n","  for music_genre in music_genres_list:\n","    genre_img_data = get_images(f\"{dataset_dir}/{music_genre}\") # get images from each directory\n","\n","    num_images = len(genre_img_data)\n","    num_classes = len(music_genres_list)\n","    ground_truths = np.zeros(shape=(num_images, num_classes)) # create empty array of 1000 images and 10 classes --> np.shape() = (1000, 10)\n","    input_data.append(genre_img_data) # add to overall list to convert to giant np array later\n","\n","    for index in range(ground_truths.shape[0]): # for each image, change the value of the 10 array and put in a 1 for its class position (genre_index)\n","     ground_truths[index, genre_index] = 1\n","\n","    target_labels.append(ground_truths) # add to overall list to convert to giant np array later again\n","    genre_index += 1 # iterate to next music genre\n","\n","  #print(input_data[9].shape)\n","  input_data = np.vstack(input_data) # concatenate all classes\n","  target_labels = np.vstack(target_labels)\n","\n","  # take an example from dataset for a sanity check\n","  example_num = 310\n","  print(f\"\\npredict data shape: {input_data.shape}\\ntarget labels shape: {target_labels.shape}\")\n","  print(f\"Example input size: {input_data[example_num].shape}\\nExample target_label {target_labels[example_num]}\")\n","\n","  return input_data, target_labels\n","\n","\n","\n","\n","spectrogram_train, spectrogram_targets = get_dataset(dataset_path)\n","print(f\"\\n\\nLoaded train dataset: {spectrogram_train.shape}, {spectrogram_targets.shape}\")"],"metadata":{"id":"sKrihvDTUQlI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save to disk"],"metadata":{"id":"UPuabH0OOj7X"}},{"cell_type":"code","source":["# save mfcc data to disk so it doesn't take so long\n","%cd \"/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\"\n","\n","# takes 18s\n","np.save('spectrogram_train.npy', spectrogram_train)\n","np.save('spectrogram_targets.npy', spectrogram_targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSsJtoZfDnbO","executionInfo":{"status":"ok","timestamp":1701780057368,"user_tz":420,"elapsed":19659,"user":{"displayName":"Adrian Ba","userId":"05340278167687070637"}},"outputId":"147fef47-d5e8-4f4a-c566-97c156cf4310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n"]}]},{"cell_type":"markdown","source":["# Load from disk"],"metadata":{"id":"tuUIy0QChsou"}},{"cell_type":"code","source":["spectrogram_train = np.load('spectrogram_train.npy')\n","spectrogram_targets = np.load('spectrogram_targets.npy')"],"metadata":{"id":"zAVJpm8htKef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make a prediction"],"metadata":{"id":"AyBaE41-O1ql"}},{"cell_type":"code","source":["# some note: look at genre: rock, track 99. Mislcassifies as Jazz, but is actually rock. Though I would think it's jazz\n","\n","data_loading_image_size = (288, 432)\n","\n","genre = \"\"\n","track_num = \"99\"\n","\n","# specify path to test song\n","example_song = f\"{genre}/{genre}000{track_num}.png\"\n","audio_file = f\"{genre}/{genre}.000{track_num}.wav\"\n","test_song_path = f\"/content/drive/MyDrive/Extended_Essay/Data_GZAN/images_original/{example_song}\"\n","test_audio_path = f\"/content/drive/MyDrive/Extended_Essay/Data_GZAN/genres_original/{audio_file}\"\n","\n","# load test song as PIL using Keras\n","test_song = keras.utils.load_img(\n","        path=test_song_path,\n","        color_mode='rgb',\n","        target_size=data_loading_image_size,\n","        interpolation='bilinear')\n","\n","test_song = trim_white_space(test_song)\n","\n","# convert test song to numpy array and performing pixel scaling\n","test_song_array = keras.utils.img_to_array(test_song)\n","test_song_array = test_song_array.astype(\"float32\") / 255  # divide pixel values by 255\n","test_song_array = np.expand_dims(test_song_array, axis=0)  # add one more dimension so batchsize = 1 --> (1, 288, 432, 3)\n","\n","# make model predictions\n","predicted_scores = spectrogram_model.predict(test_song_array)\n","predicted_music_genre = np.max(predicted_scores)\n","# find most confident prediction and index to list to find genre\n","predicted_label = np.where(predicted_scores == predicted_music_genre)[1][0]\n","predicted_music_genre = music_genres_list[predicted_label]\n","\n","# print some of the raw scores and plot the mel spectrogram graph\n","print(predicted_scores)\n","print(predicted_music_genre)\n","plt.imshow(test_song)\n","plt.show()\n","\n","# make nice audio slider, note standard sampling rate is 22-40kHz\n","sampling_rate = 22000\n","display(Audio(test_audio_path, rate=sampling_rate, autoplay=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"NQOaK7j3647G","executionInfo":{"status":"error","timestamp":1702028405358,"user_tz":420,"elapsed":556,"user":{"displayName":"Adrian Ba","userId":"05340278167687070637"}},"outputId":"2ab43087-3a72-40bc-80c1-6a3dd8460e5e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-98341d087f44>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# load test song as PIL using Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m test_song = keras.utils.load_img(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_song_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Extended_Essay/Data_GZAN/images_original//00099.png'"]}]},{"cell_type":"code","source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.title('LSTM Small Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","#plt.legend(['train'], loc='upper left')\n","plt.show()\n","\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.title('LSTM Small Accuracy')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","#plt.legend(['train'], loc='upper left')\n","plt.show()\n","\n","\n","#burrowed from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/"],"metadata":{"id":"OUntiTMteOtR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# takes 28s\n","%cd \"/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\"\n","spectrogram_train = np.load('spectrogram_train.npy')\n","spectrogram_targets = np.load('spectrogram_targets.npy')"],"metadata":{"id":"1YPlApIMOrTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load seperate genre data"],"metadata":{"id":"Dmehwg-ZwOJ5"}},{"cell_type":"code","source":["def seperate_genre_data(train_dataset):\n","  # make a placeholder that we can modify without worrying\n","  placeholder_dataset = train_dataset\n","  num_tracks_per_genre = {}\n","  genre_individual_data = {}\n","\n","  # get num of tracks per genre and save to a directory\n","  for music_genre in music_genres_list:\n","    genre_directory = f\"/content/drive/MyDrive/Extended_Essay/Data_GZAN/images_original/{music_genre}\"\n","    count = 0\n","    # Iterate directory\n","    for path in os.listdir(genre_directory):\n","        # check if current path is a file\n","        if os.path.isfile(os.path.join(genre_directory, path)):\n","            count += 1\n","\n","    # add to overall dictionary and continously modify the placeholder dataset\n","    genre_individual_data[music_genre] = placeholder_dataset[:count]\n","    placeholder_dataset = placeholder_dataset[count:]\n","\n","  return genre_individual_data\n","\n","genre_seperate_data = seperate_genre_data(spectrogram_train)"],"metadata":{"id":"TTeRybtewSso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Click for smaller model"],"metadata":{"id":"FmGm5OzjOvDY"}},{"cell_type":"code","source":["# define our model: example take from here: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n","input_dims = (218, 336, 3)\n","\n","spectrogram_model = None\n","\n","spectrogram_model = Sequential()\n","spectrogram_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_dims))\n","spectrogram_model.add(Activation('relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(32, (3, 3)))\n","spectrogram_model.add(Activation('relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(32, (3, 3)))\n","spectrogram_model.add(Activation('relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n","spectrogram_model.add(Dense(64))\n","spectrogram_model.add(Activation('relu'))\n","spectrogram_model.add(Dense(32))\n","spectrogram_model.add(Activation('relu'))\n","spectrogram_model.add(Dense(10))  # change to num of predicted classes\n","spectrogram_model.add(Activation('sigmoid'))\n","\n","\n","spectrogram_model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adam\",\n","              metrics=['accuracy'])\n","\n","\n","spectrogram_model.summary()\n"],"metadata":{"id":"IRbY5DkGdHLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 20\n","\n","class CustomEarlyStopping(Callback):\n","    def __init__(self, target_accuracy=1.0):\n","        super(CustomEarlyStopping, self).__init__()\n","        self.target_accuracy = target_accuracy\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if logs.get('accuracy') >= self.target_accuracy:\n","            print(f\"\\nReached target accuracy ({self.target_accuracy}). Training stopped.\")\n","\n","# Define custom early stopping callback\n","custom_early_stopping = CustomEarlyStopping(target_accuracy=1.0)\n","\n","#experiments with epochs or batch_size!\n","history = spectrogram_model.fit(\n","                                x=spectrogram_train,\n","                                y=spectrogram_targets,\n","                                epochs=20,\n","                                batch_size=batch_size,\n","                                validation_split=0.2,\n","                                callbacks=[custom_early_stopping]\n","                               )\n"],"metadata":{"id":"K82n-dl4pLUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Plot learning curve"],"metadata":{"id":"MaO-TUofdvUk"}},{"cell_type":"code","source":["#burrowed from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.title('CNN (Small) Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylim(0, 1.2)  # Set the y-axis limit from 0 to 1.2\n","plt.show()\n","\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.title('CNN (Small) loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","#plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"1HamajQgsrtf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create confusion matrix"],"metadata":{"id":"Tk7tPcWiUzt1"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","def get_confusion_matrix():\n","  # create some empty variables\n","  correct_classifications_dict = {}\n","  y_true = []\n","  y_predictions = []\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    genre_index = music_genres_list.index(music_genre)\n","    correct_counter = 0\n","\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","\n","    # perform predictions\n","    predictions = spectrogram_model.predict(spectrogram_test_features)\n","\n","    # Finding the index of the maximum value along each prediction scores\n","    max_indices = np.argmax(predictions, axis=1)\n","\n","    for predicted_label in max_indices:\n","      # add to correct counter if scored correctly\n","      if predicted_label == genre_index:\n","        correct_counter += 1\n","\n","      # add to previous dictionaries and list for confusion matrix later\n","      y_true.append(music_genre)\n","      y_predictions.append(music_genres_list[predicted_label])\n","\n","\n","    correct_classifications_dict[music_genre] = correct_counter\n","\n","  # create the confusion matrix using sklearn\n","  generated_conf_matrix = sklearn.metrics.confusion_matrix(y_true,y_predictions)\n","\n","  return generated_conf_matrix, correct_classifications_dict\n","\n","spec_conf_matrix, correct_dict = get_confusion_matrix()\n","print(spec_conf_matrix)\n","print(correct_dict)"],"metadata":{"id":"NhljZVucx46e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Display Confusion Matrix"],"metadata":{"id":"HqTwR_W7b7NG"}},{"cell_type":"code","source":["# Plotting the confusion matrix as a heatmap\n","plt.figure(figsize=(8, 6))\n","plt.imshow(spec_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('CNN (Small) Confusion Matrix')\n","plt.colorbar()\n","\n","tick_marks = np.arange(len(music_genres_list))\n","plt.xticks(tick_marks, music_genres_list, rotation=45)\n","plt.yticks(tick_marks, music_genres_list)\n","\n","# Annotate each cell with the numeric value\n","for i in range(len(music_genres_list)):\n","    for j in range(len(music_genres_list)):\n","        text_color = 'white' if spec_conf_matrix[i, j] > np.max(spec_conf_matrix) / 2 else 'black'\n","        plt.text(j, i, str(spec_conf_matrix[i, j]), horizontalalignment='center',\n","                 verticalalignment='center', color=text_color)\n","\n","plt.tight_layout()\n","plt.ylabel('True genre')\n","plt.xlabel('Predicted genre')\n","\n","plt.show()"],"metadata":{"id":"MZfCDFXjbq4k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get last song representations, apply TSNE and obtain centroids"],"metadata":{"id":"CWQWYMbVyUkY"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","# Define a new model to extract the the Dense(32) layer\n","second_last_layer_model = keras.models.Model(inputs=spectrogram_model.input, outputs=spectrogram_model.layers[-3].output)\n","\n","\n","def get_hidden_states():\n","  # create some empty variables\n","  output_list = []\n","  genre_track_num = {}\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","    # get vector outputs\n","    vector_outputs = second_last_layer_model.predict(spectrogram_test_features)\n","    # save to list to plot later and count how many tracks there are for each genre\n","    output_list.append(vector_outputs)\n","    genre_track_num[f\"{music_genre}\"] = vector_outputs.shape[0]\n","\n","  # flatten list of lists into numpy array\n","  outputs_array = np.vstack((output_list))\n","\n","  return genre_track_num, outputs_array\n","\n","genres_totals, raw_learned_representations = get_hidden_states()"],"metadata":{"id":"L3Q-U3Bcycx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform hidden states dimensions of 20 to 2 so we can plot it.\n","# use TSNE dimensionality reduction algorithm to reduce size from (100,330) --> (100,2)\n","%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","tsne_model = sklearn.manifold.TSNE(learning_rate='auto', perplexity=50)\n","transformed_values = tsne_model.fit_transform(raw_learned_representations)\n","\n","TSNE_dict = {}\n","\n","for music_genre in genres_totals:\n","  num_tracks = genres_totals[music_genre]\n","  TSNE_dict[music_genre] = transformed_values[:num_tracks]\n","  transformed_values = transformed_values[num_tracks:]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uchGIpIjyfXW","executionInfo":{"status":"ok","timestamp":1702030096684,"user_tz":420,"elapsed":5738,"user":{"displayName":"Adrian Ba","userId":"05340278167687070637"}},"outputId":"63a2a51e-c33c-4daa-8008-f8b461aeffdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n"]}]},{"cell_type":"code","source":["genre_centroids = {}\n","\n","# find the average central point of each genre\n","for genre in TSNE_dict:\n","  # get each data for each genre\n","  data = TSNE_dict[genre]\n","\n","  # calculate mean_x/y then add to dictionary\n","  mean_x = np.mean(data[:, 0])\n","  mean_y = np.mean(data[:, 1])\n","  centroid = np.array([mean_x, mean_y])\n","\n","  # calculate euclidean distance (also called L2 norm) from the calculate centroid\n","  total = 0\n","  for datapoint in data:\n","    distance = np.linalg.norm(centroid-datapoint)\n","    total += distance\n","\n","  # calculate average distance from centroid\n","  average_distance = total / len(data)\n","  genre_centroids[genre] = centroid"],"metadata":{"id":"koUtvXxYBM65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot learned representations"],"metadata":{"id":"fYz9nV8X0hBt"}},{"cell_type":"code","source":["# Plotting each label's data points using plotly library\n","fig = px.scatter()\n","\n","# Add traces for each label's data points\n","for label, data_points in TSNE_dict.items():\n","    fig.add_scatter(x=data_points[:, 0], y=data_points[:, 1], mode='markers', name=label)\n","\n","# configure plot layout\n","fig.update_layout(\n","    title='CNN (Small) Song Representations',\n","    xaxis_title='X-axis',\n","    yaxis_title='Y-axis'\n",")\n","\n","# Add centroids to the plot\n","for genre in genre_centroids:\n","    fig.add_trace(go.Scatter(\n","        x=[genre_centroids[genre][0]],\n","        y=[genre_centroids[genre][1]],\n","        mode='markers',\n","        marker=dict(size=15, symbol='star', line=dict(color='black', width=2)),\n","        showlegend=False\n","    ))\n","\n","fig.show()"],"metadata":{"id":"ZvlZUb_30gca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Click for bigger model\n"],"metadata":{"id":"BSB2UNFru5I1"}},{"cell_type":"code","source":["# define our model: example take from here: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n","input_dims = (218, 336, 3)\n","\n","spectrogram_model = None\n","\n","spectrogram_model = Sequential()\n","\n","# Convolutional layers\n","spectrogram_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_dims))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(64, (3, 3), activation='relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(128, (3, 3), activation='relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Flatten before dense layers\n","spectrogram_model.add(Flatten())\n","\n","# Dense layers\n","spectrogram_model.add(Dense(256, activation='relu'))\n","spectrogram_model.add(Dense(128, activation='relu'))\n","spectrogram_model.add(Dense(64, activation='relu'))\n","spectrogram_model.add(Dense(32, activation='relu'))\n","\n","# Output layer\n","num_classes = 10  # Change this to the number of predicted classes\n","spectrogram_model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","spectrogram_model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adam\",\n","              metrics=['accuracy'])\n","\n","\n","#spectrogram_model.summary()\n","plot_model(model, to_file='Mel-spectrogram (Big).png', show_shapes=True, show_layer_names=True)\n"],"metadata":{"id":"AksYkC-6u5JB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dims = (218, 336, 3)\n","\n","spectrogram_model = None\n","\n","spectrogram_model = Sequential()\n","\n","# Convolutional layers\n","spectrogram_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_dims))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(64, (3, 3), activation='relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","spectrogram_model.add(Conv2D(128, (3, 3), activation='relu'))\n","spectrogram_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Flatten before dense layers\n","spectrogram_model.add(Flatten())\n","\n","# Dense layers\n","spectrogram_model.add(Dense(256, activation='relu'))\n","spectrogram_model.add(Dense(128, activation='relu'))\n","spectrogram_model.add(Dense(64, activation='relu'))\n","spectrogram_model.add(Dense(32, activation='relu'))\n","\n","# Output layer\n","num_classes = 10  # Change this to the number of predicted classes\n","spectrogram_model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","spectrogram_model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adam\",\n","              metrics=['accuracy'])\n","\n","\n","#spectrogram_model.summary()\n","plot_model(model, to_file='Mel-spectrogram (Big).png', show_shapes=True, show_layer_names=True)\n","\n","\n","batch_size = 32\n","epochs = 20\n","\n","class CustomEarlyStopping(Callback):\n","    def __init__(self, target_accuracy=1.0):\n","        super(CustomEarlyStopping, self).__init__()\n","        self.target_accuracy = target_accuracy\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if logs.get('accuracy') >= self.target_accuracy:\n","            print(f\"\\nReached target accuracy ({self.target_accuracy}). Training stopped.\")\n","\n","# Define custom early stopping callback\n","custom_early_stopping = CustomEarlyStopping(target_accuracy=1.0)\n","\n","#experiments with epochs or batch_size!\n","history = spectrogram_model.fit(\n","                                x=spectrogram_train,\n","                                y=spectrogram_targets,\n","                                epochs=20,\n","                                batch_size=batch_size,\n","                                validation_split=0.2,\n","                                callbacks=[custom_early_stopping]\n","                               )\n"],"metadata":{"id":"yFvZ8TX4u5JC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Plot learning curve"],"metadata":{"id":"sHx6IDXgu5JC"}},{"cell_type":"code","source":["#burrowed from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.title('CNN (Big) Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","#plt.legend(['train', 'validation'], loc='upper left')\n","plt.ylim(0, 1.2)  # Set the y-axis limit from 0 to 1.2\n","plt.show()\n","\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","#plt.plot(history.history['val_loss'])\n","plt.title('CNN (Big) loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","#plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"hCHCMTjIu5JC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create confusion matrix"],"metadata":{"id":"nOZp3_zMu5JC"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","def get_confusion_matrix():\n","  # create some empty variables\n","  correct_classifications_dict = {}\n","  y_true = []\n","  y_predictions = []\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    genre_index = music_genres_list.index(music_genre)\n","    correct_counter = 0\n","\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","\n","    # perform predictions\n","    predictions = spectrogram_model.predict(spectrogram_test_features)\n","\n","    # Finding the index of the maximum value along each prediction scores\n","    max_indices = np.argmax(predictions, axis=1)\n","\n","    for predicted_label in max_indices:\n","      # add to correct counter if scored correctly\n","      if predicted_label == genre_index:\n","        correct_counter += 1\n","\n","      # add to previous dictionaries and list for confusion matrix later\n","      y_true.append(music_genre)\n","      y_predictions.append(music_genres_list[predicted_label])\n","\n","\n","    correct_classifications_dict[music_genre] = correct_counter\n","\n","  # create the confusion matrix using sklearn\n","  generated_conf_matrix = sklearn.metrics.confusion_matrix(y_true,y_predictions)\n","\n","  return generated_conf_matrix, correct_classifications_dict\n","\n","spec_conf_matrix, correct_dict = get_confusion_matrix()\n","print(spec_conf_matrix)\n","print(correct_dict)"],"metadata":{"id":"HvXSexB5u5JD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Display Confusion Matrix"],"metadata":{"id":"CL5vcFRqu5JD"}},{"cell_type":"code","source":["# Plotting the confusion matrix as a heatmap\n","plt.figure(figsize=(8, 6))\n","plt.imshow(spec_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('CNN (Big) Confusion Matrix')\n","plt.colorbar()\n","\n","tick_marks = np.arange(len(music_genres_list))\n","plt.xticks(tick_marks, music_genres_list, rotation=45)\n","plt.yticks(tick_marks, music_genres_list)\n","\n","# Annotate each cell with the numeric value\n","for i in range(len(music_genres_list)):\n","    for j in range(len(music_genres_list)):\n","        text_color = 'white' if spec_conf_matrix[i, j] > np.max(spec_conf_matrix) / 2 else 'black'\n","        plt.text(j, i, str(spec_conf_matrix[i, j]), horizontalalignment='center',\n","                 verticalalignment='center', color=text_color)\n","\n","plt.tight_layout()\n","plt.ylabel('True genre')\n","plt.xlabel('Predicted genre')\n","\n","plt.show()"],"metadata":{"id":"_GPKTe3uu5JD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get last song representations, apply TSNE and obtain centroids"],"metadata":{"id":"3sDasCP_u5JE"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","# Define a new model to extract the the Dense(32) layer\n","second_last_layer_model = keras.models.Model(inputs=spectrogram_model.input, outputs=spectrogram_model.layers[-3].output)\n","\n","\n","def get_hidden_states():\n","  # create some empty variables\n","  output_list = []\n","  genre_track_num = {}\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","    # get vector outputs\n","    vector_outputs = second_last_layer_model.predict(spectrogram_test_features)\n","    # save to list to plot later and count how many tracks there are for each genre\n","    output_list.append(vector_outputs)\n","    genre_track_num[f\"{music_genre}\"] = vector_outputs.shape[0]\n","\n","  # flatten list of lists into numpy array\n","  outputs_array = np.vstack((output_list))\n","\n","  return genre_track_num, outputs_array\n","\n","genres_totals, raw_learned_representations = get_hidden_states()"],"metadata":{"id":"MDZQfpsVu5JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform hidden states dimensions of 20 to 2 so we can plot it.\n","# use TSNE dimensionality reduction algorithm to reduce size from (100,330) --> (100,2)\n","%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","tsne_model = sklearn.manifold.TSNE(learning_rate='auto', perplexity=50)\n","transformed_values = tsne_model.fit_transform(raw_learned_representations)\n","\n","TSNE_dict = {}\n","\n","for music_genre in genres_totals:\n","  num_tracks = genres_totals[music_genre]\n","  TSNE_dict[music_genre] = transformed_values[:num_tracks]\n","  transformed_values = transformed_values[num_tracks:]\n"],"metadata":{"id":"R9j7ceWwu5JE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702030201388,"user_tz":420,"elapsed":7056,"user":{"displayName":"Adrian Ba","userId":"05340278167687070637"}},"outputId":"8c6be7d5-4c30-4453-b658-dc88f5d2439c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n"]}]},{"cell_type":"code","source":["genre_centroids = {}\n","\n","# find the average central point of each genre\n","for genre in TSNE_dict:\n","  # get each data for each genre\n","  data = TSNE_dict[genre]\n","\n","  # calculate mean_x/y then add to dictionary\n","  mean_x = np.mean(data[:, 0])\n","  mean_y = np.mean(data[:, 1])\n","  centroid = np.array([mean_x, mean_y])\n","\n","  # calculate euclidean distance (also called L2 norm) from the calculate centroid\n","  total = 0\n","  for datapoint in data:\n","    distance = np.linalg.norm(centroid-datapoint)\n","    total += distance\n","\n","  # calculate average distance from centroid\n","  average_distance = total / len(data)\n","  genre_centroids[genre] = centroid"],"metadata":{"id":"V_n_Gkg4u5JF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot learned representations"],"metadata":{"id":"LElfXk29u5JF"}},{"cell_type":"code","source":["# Plotting each label's data points using plotly library\n","fig = px.scatter()\n","\n","# Add traces for each label's data points\n","for label, data_points in TSNE_dict.items():\n","    fig.add_scatter(x=data_points[:, 0], y=data_points[:, 1], mode='markers', name=label)\n","\n","# configure plot layout\n","fig.update_layout(\n","    title='CNN (Big) Song Representations',\n","    xaxis_title='X-axis',\n","    yaxis_title='Y-axis'\n",")\n","\n","# Add centroids to the plot\n","for genre in genre_centroids:\n","    fig.add_trace(go.Scatter(\n","        x=[genre_centroids[genre][0]],\n","        y=[genre_centroids[genre][1]],\n","        mode='markers',\n","        marker=dict(size=15, symbol='star', line=dict(color='black', width=2)),\n","        showlegend=False\n","    ))\n","\n","fig.show()"],"metadata":{"id":"jluxBFu-u5JF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Click for pre-trained model"],"metadata":{"id":"kJeCM50TvBbW"}},{"cell_type":"code","source":["# define our model: example take from here: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n","input_dims = (218, 336, 3)\n","\n","# Load the VGG model without the top classification layers\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_dims)\n","\n","# Freeze the layers of the base model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Add new classification layers\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(256, activation='relu')(x)\n","predictions = Dense(10, activation='softmax')(x)  # 10 classes\n","\n","# Create the new model\n","spectrogram_model = keras.Model(inputs=base_model.input, outputs=predictions)\n","\n","\n","spectrogram_model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adam\",\n","              metrics=['accuracy'])\n","\n","\n","spectrogram_model.summary()\n"],"metadata":{"id":"fN8eXax4vBbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 20\n","\n","class CustomEarlyStopping(Callback):\n","    def __init__(self, target_accuracy=1.0):\n","        super(CustomEarlyStopping, self).__init__()\n","        self.target_accuracy = target_accuracy\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if logs.get('accuracy') >= self.target_accuracy:\n","            print(f\"\\nReached target accuracy ({self.target_accuracy}). Training stopped.\")\n","\n","# Define custom early stopping callback\n","custom_early_stopping = CustomEarlyStopping(target_accuracy=1.0)\n","\n","#experiments with epochs or batch_size!\n","history = spectrogram_model.fit(\n","                                x=spectrogram_train,\n","                                y=spectrogram_targets,\n","                                epochs=20,\n","                                batch_size=batch_size,\n","                                validation_split=0.2,\n","                                callbacks=[custom_early_stopping]\n","                               )\n"],"metadata":{"id":"MKgYNR8gvBbq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Plot learning curve"],"metadata":{"id":"CAeH2NChvBbr"}},{"cell_type":"code","source":["#burrowed from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","# summarize history for accuracy\n","# Plot accuracy\n","plt.plot(history.history['accuracy'])\n","plt.title('CNN (Pre-trained) Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylim(0, 1.2)  # Set the y-axis limit from 0 to 1.2\n","plt.show()\n","\n","# Plot loss\n","plt.plot(history.history['loss'])\n","plt.title('CNN (Pre-trained) Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.show()"],"metadata":{"id":"klQw53j_vBbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create confusion matrix"],"metadata":{"id":"UYIRGs2svBbr"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","def get_confusion_matrix():\n","  # create some empty variables\n","  correct_classifications_dict = {}\n","  y_true = []\n","  y_predictions = []\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    genre_index = music_genres_list.index(music_genre)\n","    correct_counter = 0\n","\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","\n","    # perform predictions\n","    predictions = spectrogram_model.predict(spectrogram_test_features)\n","\n","    # Finding the index of the maximum value along each prediction scores\n","    max_indices = np.argmax(predictions, axis=1)\n","\n","    for predicted_label in max_indices:\n","      # add to correct counter if scored correctly\n","      if predicted_label == genre_index:\n","        correct_counter += 1\n","\n","      # add to previous dictionaries and list for confusion matrix later\n","      y_true.append(music_genre)\n","      y_predictions.append(music_genres_list[predicted_label])\n","\n","\n","    correct_classifications_dict[music_genre] = correct_counter\n","\n","  # create the confusion matrix using sklearn\n","  generated_conf_matrix = sklearn.metrics.confusion_matrix(y_true,y_predictions)\n","\n","  return generated_conf_matrix, correct_classifications_dict\n","\n","spec_conf_matrix, correct_dict = get_confusion_matrix()\n","print(spec_conf_matrix)\n","print(correct_dict)"],"metadata":{"id":"3NPiaR_5vBbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Display Confusion Matrix"],"metadata":{"id":"F-UNl8ivvBbr"}},{"cell_type":"code","source":["# Plotting the confusion matrix as a heatmap\n","plt.figure(figsize=(8, 6))\n","plt.imshow(spec_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('CNN (Pre-trained) Confusion Matrix')\n","plt.colorbar()\n","\n","tick_marks = np.arange(len(music_genres_list))\n","plt.xticks(tick_marks, music_genres_list, rotation=45)\n","plt.yticks(tick_marks, music_genres_list)\n","\n","# Annotate each cell with the numeric value\n","for i in range(len(music_genres_list)):\n","    for j in range(len(music_genres_list)):\n","        text_color = 'white' if spec_conf_matrix[i, j] > np.max(spec_conf_matrix) / 2 else 'black'\n","        plt.text(j, i, str(spec_conf_matrix[i, j]), horizontalalignment='center',\n","                 verticalalignment='center', color=text_color)\n","\n","plt.tight_layout()\n","plt.ylabel('True genre')\n","plt.xlabel('Predicted genre')\n","\n","plt.show()"],"metadata":{"id":"jkmpt9_KvBbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get last song representations, apply TSNE and obtain centroids"],"metadata":{"id":"r1KnO2BmvBbs"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","# Define a new model to extract the the Dense(32) layer\n","second_last_layer_model = keras.models.Model(inputs=spectrogram_model.input, outputs=spectrogram_model.layers[-2].output)\n","\n","\n","def get_hidden_states():\n","  # create some empty variables\n","  output_list = []\n","  genre_track_num = {}\n","\n","  # recall the seperate data we loaded from the start\n","  for music_genre in genre_seperate_data:\n","    # load data from that seperate dataset we created earlier\n","    spectrogram_test_features = genre_seperate_data[music_genre]\n","    # get vector outputs\n","    vector_outputs = second_last_layer_model.predict(spectrogram_test_features)\n","    # save to list to plot later and count how many tracks there are for each genre\n","    output_list.append(vector_outputs)\n","    genre_track_num[f\"{music_genre}\"] = vector_outputs.shape[0]\n","\n","  # flatten list of lists into numpy array\n","  outputs_array = np.vstack((output_list))\n","\n","  return genre_track_num, outputs_array\n","\n","genres_totals, raw_learned_representations = get_hidden_states()"],"metadata":{"id":"LVIXTiktvBbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transform hidden states dimensions of 20 to 2 so we can plot it.\n","# use TSNE dimensionality reduction algorithm to reduce size from (100,330) --> (100,2)\n","%cd /content/drive/MyDrive/Extended_Essay/Data_GZAN/spectrogram_data\n","\n","tsne_model = sklearn.manifold.TSNE(learning_rate='auto', perplexity=50)\n","transformed_values = tsne_model.fit_transform(raw_learned_representations)\n","\n","TSNE_dict = {}\n","\n","for music_genre in genres_totals:\n","  num_tracks = genres_totals[music_genre]\n","  TSNE_dict[music_genre] = transformed_values[:num_tracks]\n","  transformed_values = transformed_values[num_tracks:]\n"],"metadata":{"id":"9EKN7LXTvBbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre_centroids = {}\n","\n","# find the average central point of each genre\n","for genre in TSNE_dict:\n","  # get each data for each genre\n","  data = TSNE_dict[genre]\n","\n","  # calculate mean_x/y then add to dictionary\n","  mean_x = np.mean(data[:, 0])\n","  mean_y = np.mean(data[:, 1])\n","  centroid = np.array([mean_x, mean_y])\n","\n","  # calculate euclidean distance (also called L2 norm) from the calculate centroid\n","  total = 0\n","  for datapoint in data:\n","    distance = np.linalg.norm(centroid-datapoint)\n","    total += distance\n","\n","  # calculate average distance from centroid\n","  average_distance = total / len(data)\n","  genre_centroids[genre] = centroid"],"metadata":{"id":"Iddt0gf6vBbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting each label's data points using plotly library\n","fig = px.scatter()\n","\n","# Add traces for each label's data points\n","for label, data_points in TSNE_dict.items():\n","    fig.add_scatter(x=data_points[:, 0], y=data_points[:, 1], mode='markers', name=label)\n","\n","# configure plot layout\n","fig.update_layout(\n","    title='CNN (Pre-trained) Song Representations',\n","    xaxis_title='X-axis',\n","    yaxis_title='Y-axis'\n",")\n","\n","# Add centroids to the plot\n","for genre in genre_centroids:\n","    fig.add_trace(go.Scatter(\n","        x=[genre_centroids[genre][0]],\n","        y=[genre_centroids[genre][1]],\n","        mode='markers',\n","        marker=dict(size=15, symbol='star', line=dict(color='black', width=2)),\n","        showlegend=False\n","    ))\n","\n","fig.show()"],"metadata":{"id":"lxJc-uWWvBbs"},"execution_count":null,"outputs":[]}]}